"""
Conversation Pipeline - Phase 4
Chains Audio → ASR → LLM → TTS → Video for interactive conversations
"""

import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import time

from runtime.models.asr import ASRModel
from runtime.models.llm import LLMModel
from runtime.pipelines.phase1_script import run_pipeline

logger = logging.getLogger(__name__)


class ConversationPipeline:
    """
    Orchestrates the full conversation pipeline:
    1. Audio Input → ASR (transcription)
    2. Text → LLM (response generation)
    3. Response Text → TTS (voice cloning)
    4. TTS Audio → Video (lip-synced avatar)
    """

    def __init__(
        self,
        reference_image: str,
        reference_audio: str,
        output_dir: str = "outputs/conversations",
        device: str = "cuda",
        use_tensorrt: bool = True,
    ):
        """
        Initialize the conversation pipeline.

        Args:
            reference_image: Path to reference avatar image
            reference_audio: Path to reference voice sample for cloning
            output_dir: Directory to save conversation outputs
            device: Device for inference ('cuda', 'mps', or 'cpu')
            use_tensorrt: Whether to use TensorRT for video generation
        """
        self.reference_image = reference_image
        self.reference_audio = reference_audio
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = device
        self.use_tensorrt = use_tensorrt

        # Models (lazy loaded)
        self.asr_model: Optional[ASRModel] = None
        self.llm_model: Optional[LLMModel] = None

        # System prompt for conversational LLM
        self.system_prompt = """You are Bruce, a helpful and friendly AI assistant. 
Keep your responses concise (1-3 sentences) and conversational. 
Be natural, warm, and engaging in your communication style."""

        logger.info(f"ConversationPipeline initialized (device={device}, tensorrt={use_tensorrt})")

    def initialize(self):
        """Load all models into memory."""
        start_time = time.time()
        logger.info("Initializing conversation pipeline models...")

        # Initialize ASR
        if self.asr_model is None:
            logger.info("Loading ASR model (Faster-Whisper)...")
            self.asr_model = ASRModel(device=self.device)
            self.asr_model.initialize()

        # Initialize LLM
        if self.llm_model is None:
            logger.info("Loading LLM model (Qwen-2.5-7B)...")
            self.llm_model = LLMModel(device="cpu")  # LLM on CPU for MVP
            self.llm_model.initialize()

        # Note: TTS and Video models are loaded on-demand by phase1_script.run_pipeline()

        elapsed = time.time() - start_time
        logger.info(f"All models initialized in {elapsed:.2f}s")

    def transcribe(self, audio_path: str, language: str = "en") -> Dict[str, Any]:
        """
        Transcribe audio to text using ASR.

        Args:
            audio_path: Path to audio file
            language: Language code (en, zh, es)

        Returns:
            Dict with 'text', 'language', and timing info
        """
        if self.asr_model is None:
            raise RuntimeError("Pipeline not initialized. Call initialize() first.")

        start_time = time.time()
        logger.info(f"Transcribing audio: {audio_path}")

        result = self.asr_model.transcribe(audio_path, language=language)
        result["transcribe_time"] = time.time() - start_time

        logger.info(f"Transcription: '{result['text'][:100]}...' ({result['transcribe_time']:.2f}s)")
        return result

    def generate_response(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        max_tokens: int = 150,
    ) -> Dict[str, Any]:
        """
        Generate LLM response to user message.

        Args:
            user_message: User's input text
            conversation_history: Optional list of {'role': 'user'/'assistant', 'content': '...'}
            max_tokens: Maximum response length

        Returns:
            Dict with 'response' text and timing info
        """
        if self.llm_model is None:
            raise RuntimeError("Pipeline not initialized. Call initialize() first.")

        start_time = time.time()
        logger.info(f"Generating LLM response for: '{user_message[:100]}...'")

        if conversation_history:
            response = self.llm_model.generate_with_history(
                messages=conversation_history,
                system_prompt=self.system_prompt,
                max_new_tokens=max_tokens,
            )
        else:
            response = self.llm_model.generate_response(
                prompt=user_message,
                system_prompt=self.system_prompt,
                max_new_tokens=max_tokens,
            )

        result = {
            "response": response,
            "llm_time": time.time() - start_time,
        }

        logger.info(f"LLM response: '{response[:100]}...' ({result['llm_time']:.2f}s)")
        return result

    def generate_avatar_video(
        self,
        text: str,
        output_name: str,
        language: str = "en",
    ) -> Dict[str, Any]:
        """
        Generate avatar video from text using TTS + Video pipeline.

        Args:
            text: Text to speak
            output_name: Base name for output files (without extension)
            language: Language code

        Returns:
            Dict with 'video_path', 'audio_path', and timing info
        """
        start_time = time.time()
        logger.info(f"Generating avatar video for: '{text[:100]}...'")

        # Use existing phase1_script pipeline (TTS + Video)
        output_dir = str(self.output_dir / output_name)
        
        result = run_pipeline(
            text=text,
            reference_image=self.reference_image,
            reference_audio=self.reference_audio,
            output_dir=output_dir,
            language=language,
            device=self.device,
            use_tensorrt=self.use_tensorrt,
        )

        result["total_generation_time"] = time.time() - start_time
        logger.info(f"Avatar video generated: {result.get('video_path')} ({result['total_generation_time']:.2f}s)")

        return result

    def process_conversation(
        self,
        audio_path: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        output_name: Optional[str] = None,
        language: str = "en",
    ) -> Dict[str, Any]:
        """
        Full conversation pipeline: Audio → ASR → LLM → TTS → Video.

        Args:
            audio_path: Path to user's audio input
            conversation_history: Optional conversation context
            output_name: Base name for outputs (auto-generated if None)
            language: Language code

        Returns:
            Dict with all results:
            - transcription: ASR result
            - llm_response: LLM result
            - avatar_video: Video generation result
            - total_time: End-to-end time
        """
        if self.asr_model is None or self.llm_model is None:
            raise RuntimeError("Pipeline not initialized. Call initialize() first.")

        pipeline_start = time.time()
        logger.info(f"Processing conversation from audio: {audio_path}")

        # Step 1: Transcribe user audio
        transcription = self.transcribe(audio_path, language=language)
        user_text = transcription["text"]

        # Step 2: Generate LLM response
        llm_result = self.generate_response(
            user_message=user_text,
            conversation_history=conversation_history,
        )
        response_text = llm_result["response"]

        # Step 3: Generate avatar video
        if output_name is None:
            output_name = f"conversation_{int(time.time())}"

        avatar_result = self.generate_avatar_video(
            text=response_text,
            output_name=output_name,
            language=language,
        )

        # Compile results
        total_time = time.time() - pipeline_start
        result = {
            "transcription": transcription,
            "llm_response": llm_result,
            "avatar_video": avatar_result,
            "total_time": total_time,
            "user_text": user_text,
            "response_text": response_text,
        }

        logger.info(f"Conversation processed in {total_time:.2f}s")
        logger.info(f"  User: '{user_text[:80]}...'")
        logger.info(f"  Assistant: '{response_text[:80]}...'")
        logger.info(f"  Video: {avatar_result.get('video_path')}")

        return result
