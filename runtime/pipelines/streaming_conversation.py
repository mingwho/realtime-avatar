"""
Streaming Conversation Pipeline
Generates and streams video chunks as they're produced for lower perceived latency.
"""

import logging
import time
import asyncio
import os
from pathlib import Path
from typing import Optional, Dict, Any, List, AsyncGenerator
import re

from models.asr import ASRModel
from models.llm import LLMModel
from models.llm_gemini import GeminiClient
from pipelines.phase1_script import Phase1Pipeline
from config import settings

logger = logging.getLogger(__name__)


async def ensure_video_fully_written(video_path: str, max_wait: float = 3.0) -> bool:
    """
    Wait for video file to be fully written with stable size and fsync.
    
    This prevents race conditions where the video file exists but isn't fully flushed to disk,
    causing the video endpoint to serve partial/corrupted files.
    
    Args:
        video_path: Absolute path to video file
        max_wait: Maximum time to wait in seconds
        
    Returns:
        True if file is ready, False if timeout
    """
    wait_start = time.time()
    
    # Wait for file to exist
    while not os.path.exists(video_path):
        if time.time() - wait_start > max_wait:
            logger.warning(f"[FSYNC] Timeout waiting for file to exist: {video_path}")
            return False
        await asyncio.sleep(0.05)  # 50ms check interval
    
    # Wait for stable file size (check twice 100ms apart)
    prev_size = -1
    curr_size = os.path.getsize(video_path)
    stable_checks = 0
    required_stable_checks = 2  # File size must be stable for 2 checks
    
    while stable_checks < required_stable_checks:
        if time.time() - wait_start > max_wait:
            logger.warning(f"[FSYNC] Timeout waiting for stable size: {video_path} (curr_size={curr_size})")
            return False
        
        await asyncio.sleep(0.1)  # 100ms between checks
        prev_size = curr_size
        curr_size = os.path.getsize(video_path)
        
        if prev_size == curr_size:
            stable_checks += 1
        else:
            stable_checks = 0  # Reset if size changed
    
    # Force fsync to ensure file is on disk
    try:
        with open(video_path, 'rb') as f:
            os.fsync(f.fileno())
    except Exception as e:
        logger.warning(f"[FSYNC] fsync failed for {video_path}: {e}")
    
    wait_time = time.time() - wait_start
    logger.info(f"[FSYNC] File ready: {os.path.basename(video_path)} (waited {wait_time:.3f}s, size={curr_size}, stable_checks={required_stable_checks})")
    return True


class StreamingConversationPipeline:
    """
    Streaming conversation pipeline that generates video chunks progressively.
    
    Key improvements over standard pipeline:
    - Splits LLM response into sentence chunks
    - Generates TTS + Avatar for each chunk in parallel
    - Streams video chunks to client as they're ready
    - Reduces time-to-first-frame from ~30s to ~5-6s
    """

    def __init__(
        self,
        reference_image: str,
        reference_audio: str,
        output_dir: str = "outputs/conversations",
        device: str = "cuda",
        use_tensorrt: bool = True,
        max_parallel_chunks: int = 1,  # Must be 1: GPU service processes requests serially
    ):
        """
        Initialize streaming conversation pipeline.

        Args:
            reference_image: Path to reference avatar image
            reference_audio: Path to reference voice sample for cloning
            output_dir: Directory to save conversation outputs
            device: Device for inference ('cuda', 'mps', or 'cpu')
            use_tensorrt: Whether to use TensorRT for video generation
            max_parallel_chunks: Must be 1. GPU service runs single-worker uvicorn and processes
                requests serially. L4 GPU cannot handle concurrent Ditto instances due to CUDA
                context limitations. Parallel requests would just queue at GPU service anyway.
        """
        self.reference_image = reference_image
        self.reference_audio = reference_audio
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = device
        self.use_tensorrt = use_tensorrt
        self.max_parallel_chunks = max_parallel_chunks

        # Models (lazy loaded)
        self.asr_model: Optional[ASRModel] = None
        self.llm_model: Optional[LLMModel] = None
        self.gemini_client: Optional[GeminiClient] = None
        self.phase1_pipeline: Optional[Phase1Pipeline] = None

        # System prompt for conversational LLM
        self.system_prompt = """You are Bruce, a helpful and friendly AI assistant. 
Keep your responses concise (2-4 sentences) and conversational. 
Be natural, warm, and engaging in your communication style."""

        logger.info(f"StreamingConversationPipeline initialized (max_parallel={max_parallel_chunks})")

    def initialize(self):
        """Load all models into memory."""
        start_time = time.time()
        logger.info("Initializing streaming conversation pipeline models...")

        # Initialize ASR
        if self.asr_model is None:
            logger.info("Loading ASR model (Faster-Whisper)...")
            compute_type = "int8" if self.device == "cpu" else "float16"
            self.asr_model = ASRModel(device=self.device, compute_type=compute_type)
            self.asr_model.initialize()

        # Initialize LLM (Gemini API or local Qwen)
        if settings.use_gemini_llm:
            if self.gemini_client is None:
                try:
                    logger.info(f"Initializing Gemini LLM: {settings.gemini_model}")
                    self.gemini_client = GeminiClient(
                        model_name=settings.gemini_model,
                        project_id=settings.gemini_project,
                        location=settings.gemini_location
                    )
                    self.gemini_client.initialize()
                except Exception as e:
                    logger.warning(f"Failed to load Gemini, will use fallback: {e}")
                    self.gemini_client = None
        else:
            # Use local Qwen model
            if self.llm_model is None:
                try:
                    logger.info("Loading LLM model (Qwen-2.5-7B)...")
                    self.llm_model = LLMModel()
                    self.llm_model.initialize()
                except Exception as e:
                    logger.warning(f"Failed to load LLM, will use fallback: {e}")
                    self.llm_model = None

        # Initialize Phase1Pipeline
        if self.phase1_pipeline is None:
            self.phase1_pipeline = Phase1Pipeline()
            self.phase1_pipeline.initialize()

        elapsed = time.time() - start_time
        logger.info(f"Streaming pipeline initialized in {elapsed:.2f}s")

    def split_into_sentences(self, text: str, max_chars: int = 120) -> List[str]:
        """
        Split text into sentence chunks for streaming.
        Handles common abbreviations like D.C., Mr., Dr., etc.
        Splits on periods, semicolons, and enforces max character limit.
        
        Args:
            text: Text to split
            max_chars: Maximum characters per chunk (default 120 for ~8-10s video)
            
        Returns:
            List of sentence strings
        """
        # Handle common abbreviations by temporarily replacing periods
        abbreviations = {
            'D.C.': 'DC_TEMP',
            'Mr.': 'MR_TEMP',
            'Mrs.': 'MRS_TEMP',
            'Ms.': 'MS_TEMP',
            'Dr.': 'DR_TEMP',
            'Jr.': 'JR_TEMP',
            'Sr.': 'SR_TEMP',
            'U.S.': 'US_TEMP',
            'U.K.': 'UK_TEMP',
            'etc.': 'ETC_TEMP',
            'vs.': 'VS_TEMP',
            'e.g.': 'EG_TEMP',
            'i.e.': 'IE_TEMP',
        }
        
        # Replace abbreviations temporarily
        protected_text = text
        replacements = {}
        for abbr, temp in abbreviations.items():
            if abbr in protected_text:
                protected_text = protected_text.replace(abbr, temp)
                replacements[temp] = abbr
        
        # Split on sentence boundaries (.!?;) followed by space or end
        # Semicolons are included to handle poetry and complex prose
        sentences = re.split(r'([.!?;]+(?:\s+|$))', protected_text)
        
        # Rejoin sentences with their punctuation
        chunks = []
        for i in range(0, len(sentences) - 1, 2):
            sentence = sentences[i].strip()
            punctuation = sentences[i + 1].strip() if i + 1 < len(sentences) else ''
            if sentence:
                chunk = sentence + punctuation
                # Restore abbreviations
                for temp, abbr in replacements.items():
                    chunk = chunk.replace(temp, abbr)
                chunks.append(chunk)
        
        # Handle any remaining text
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            remaining = sentences[-1].strip()
            # Restore abbreviations
            for temp, abbr in replacements.items():
                remaining = remaining.replace(temp, abbr)
            chunks.append(remaining)
        
        # Filter out empty chunks and very short ones (< 3 words)
        chunks = [c for c in chunks if len(c.split()) >= 3]
        
        # BUFFERING STRATEGY: Combine first chunks to reach ~120 chars
        # This gives pipeline time to build buffer while first video plays
        if len(chunks) >= 2:
            combined_first = chunks[0]
            chunks_combined = 1
            
            # Keep adding chunks until we reach ~120 chars
            while chunks_combined < len(chunks) and len(combined_first) < 120:
                next_chunk = chunks[chunks_combined]
                if len(combined_first) + len(next_chunk) + 1 <= 125:  # +1 for space, 125 hard limit
                    combined_first += ' ' + next_chunk
                    chunks_combined += 1
                else:
                    break
            
            # Replace first N chunks with combined version
            if chunks_combined > 1:
                chunks = [combined_first] + chunks[chunks_combined:]
                logger.info(f"BUFFERING: Combined first {chunks_combined} chunks into one ({len(combined_first)} chars)")
        
        # Enforce max character limit by splitting long chunks
        # First chunk already combined above, remaining chunks use 120 char limit
        final_chunks = []
        for idx, chunk in enumerate(chunks):
            # First chunk was already combined to ~120 chars, keep as-is if reasonable
            if idx == 0 and len(chunk) <= 125:
                final_chunks.append(chunk)
            elif len(chunk) <= max_chars:
                final_chunks.append(chunk)
            else:
                # Split long chunk at word boundaries
                words = chunk.split()
                current_chunk = []
                current_length = 0
                
                for word in words:
                    word_length = len(word) + 1  # +1 for space
                    if current_length + word_length > max_chars and current_chunk:
                        # Flush current chunk
                        final_chunks.append(' '.join(current_chunk))
                        current_chunk = [word]
                        current_length = len(word)
                    else:
                        current_chunk.append(word)
                        current_length += word_length
                
                # Add remaining words
                if current_chunk:
                    final_chunks.append(' '.join(current_chunk))
        
        # Log chunks with buffer indication
        chunk_info = []
        for i, c in enumerate(final_chunks):
            label = "BUFFER" if i == 0 else "NORMAL"
            chunk_info.append(f'[{label}] "{c[:40]}..." ({len(c)} chars)')
        logger.info(f"Split text into {len(final_chunks)} chunks: {chunk_info}")
        return final_chunks

    async def generate_chunk(
        self,
        text_chunk: str,
        chunk_index: int,
        job_id: str,
        language: str = "en",
        reference_image: Optional[str] = None,
        voice_sample: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Generate a single video chunk from text.
        
        Args:
            text_chunk: Text to generate
            chunk_index: Index of this chunk
            job_id: Base job identifier
            language: Language code
            
        Returns:
            Dict with chunk results
        """
        chunk_start = time.time()
        chunk_id = f"{job_id}_chunk{chunk_index}"
        
        logger.info(f"[{chunk_id}] Generating chunk: '{text_chunk[:50]}...'")
        
        try:
            # Generate TTS + Avatar for this chunk
            result = await self.phase1_pipeline.generate(
                text=text_chunk,
                language=language,
                reference_image=reference_image or self.reference_image,
                voice_sample=voice_sample or self.reference_audio,
                job_id=chunk_id,
            )
            
            chunk_time = time.time() - chunk_start
            result["chunk_index"] = chunk_index
            result["chunk_time"] = chunk_time
            result["text_chunk"] = text_chunk
            
            # Ensure video file is fully written to disk before returning
            video_path = result.get("video_path")
            if video_path:
                fsync_start = time.time()
                file_ready = await ensure_video_fully_written(video_path, max_wait=3.0)
                fsync_time = time.time() - fsync_start
                
                if not file_ready:
                    logger.warning(f"[{chunk_id}] Video file sync timeout after {fsync_time:.3f}s: {video_path}")
                else:
                    logger.info(f"[{chunk_id}] Video file verified (fsync took {fsync_time:.3f}s)")
            
            logger.info(f"[{chunk_id}] Chunk generated in {chunk_time:.2f}s (total with fsync: {time.time() - chunk_start:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"[{chunk_id}] Chunk generation failed: {e}", exc_info=True)
            raise

    async def process_conversation_streaming(
        self,
        audio_path: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        job_id: Optional[str] = None,
        language: str = "en",
        reference_image: Optional[str] = None,
        voice_sample: Optional[str] = None,
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stream conversation processing with progressive chunk generation.
        
        Args:
            audio_path: Path to user's audio input
            conversation_history: Optional conversation context
            job_id: Base name for outputs (auto-generated if None)
            language: Language code
            
        Yields:
            Dict with chunk results as they're generated:
            - type: "transcription" | "llm_response" | "video_chunk" | "complete"
            - data: Type-specific data
        """
        if self.asr_model is None:
            raise RuntimeError("Pipeline not initialized. Call initialize() first.")

        if job_id is None:
            job_id = f"stream_{int(time.time())}"

        pipeline_start = time.time()
        logger.info(f"[{job_id}] Starting streaming conversation processing")

        try:
            # Step 1: Transcribe user audio
            transcription_start = time.time()
            transcribe_result = self.asr_model.transcribe(audio_path, language=language)
            
            if isinstance(transcribe_result, tuple):
                if len(transcribe_result) == 3:
                    text, detected_lang, confidence = transcribe_result
                    metadata = {"language": detected_lang, "language_probability": confidence}
                else:
                    text, metadata = transcribe_result
            else:
                raise TypeError(f"Unexpected transcribe result type: {type(transcribe_result)}")
            
            transcription_time = time.time() - transcription_start
            user_text = text
            
            # Yield transcription result
            yield {
                "type": "transcription",
                "data": {
                    "text": user_text,
                    "language": metadata.get("language", language),
                    "time": transcription_time,
                }
            }
            
            logger.info(f"[{job_id}] Transcription: '{user_text[:80]}...'")

            # Step 2: Generate LLM response
            llm_start = time.time()
            
            # Check which LLM to use (Gemini or local)
            llm_available = self.gemini_client or self.llm_model
            
            if not llm_available:
                # Fallback: echo user text
                response_text = user_text
                fallback = True
            else:
<<<<<<< HEAD
                if conversation_history:
                    response_text = self.llm_model.generate_with_history(
                        messages=conversation_history,
                        system_prompt=system_prompt or self.system_prompt,
                        max_new_tokens=150,
                    )
                else:
                    response_text = self.llm_model.generate_response(
                        prompt=user_text,
                        system_prompt=system_prompt or self.system_prompt,
                        max_new_tokens=150,
                    )
                fallback = False
=======
                # Use Gemini if available, otherwise use local Qwen
                if self.gemini_client:
                    if conversation_history:
                        response_text = self.gemini_client.generate_with_history(
                            prompt=user_text,
                            conversation_history=conversation_history,
                            max_tokens=150,
                        )
                    else:
                        response_text = self.gemini_client.generate_response(
                            prompt=user_text,
                            max_tokens=150,
                        )
                    fallback = False
                else:
                    # Use local LLM
                    if conversation_history:
                        response_text = self.llm_model.generate_with_history(
                            messages=conversation_history,
                            system_prompt=self.system_prompt,
                            max_new_tokens=150,
                        )
                    else:
                        response_text = self.llm_model.generate_response(
                            prompt=user_text,
                            system_prompt=self.system_prompt,
                            max_new_tokens=150,
                        )
                    fallback = False
>>>>>>> upstream/main
            
            llm_time = time.time() - llm_start
            
            # Yield LLM response
            yield {
                "type": "llm_response",
                "data": {
                    "text": response_text,
                    "time": llm_time,
                    "fallback": fallback,
                }
            }
            
            logger.info(f"[{job_id}] LLM response: '{response_text[:80]}...'")

            # Step 3: Split response into chunks and generate video progressively
            chunks = self.split_into_sentences(response_text)
            
            if not chunks:
                # If splitting failed, use full text as single chunk
                chunks = [response_text]
            
            # Generate chunks sequentially and yield as each completes
            # Sequential processing required due to GPU service limitations
            for i, text_chunk in enumerate(chunks):
                # Generate chunk (blocks until complete)
                result = await self.generate_chunk(
                    text_chunk=text_chunk,
                    chunk_index=i,
                    job_id=job_id,
                    language=language,
                    reference_image=reference_image,
                    voice_sample=voice_sample,
                )
                
                # Yield immediately after generation completes
                yield {
                    "type": "video_chunk",
                    "data": result,
                }

            # Yield completion
            total_time = time.time() - pipeline_start
            yield {
                "type": "complete",
                "data": {
                    "total_time": total_time,
                    "num_chunks": len(chunks),
                    "user_text": user_text,
                    "response_text": response_text,
                }
            }
            
            logger.info(f"[{job_id}] Streaming conversation completed in {total_time:.2f}s")

        except Exception as e:
            logger.error(f"[{job_id}] Streaming conversation failed: {e}", exc_info=True)
            yield {
                "type": "error",
                "data": {
                    "error": str(e),
                    "time": time.time() - pipeline_start,
                }
            }
