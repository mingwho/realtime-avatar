services:
  # GPU Acceleration Service (TTS + Avatar generation)
  gpu-service:
    build:
      context: ./runtime
      dockerfile: Dockerfile.gpu-service
      args:
        CUDA: "${USE_CUDA:-false}"  # Set USE_CUDA=true for NVIDIA GPUs
    container_name: realtime-avatar-gpu
    ports:
      - "8001:8001"
    volumes:
      # Share assets with host and other services
      - ./runtime/assets:/app/assets:ro
      - ./runtime/SadTalker/checkpoints:/app/SadTalker/checkpoints
      - ./runtime/gfpgan/weights:/app/gfpgan/weights
      # Model cache for downloads
      - model-cache:/root/.cache
      # Shared output directory
      - gpu-output:/tmp/gpu-service-output
    environment:
      - PORT=8001
      - HOST=0.0.0.0
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - COQUI_TOS_AGREED=1  # Auto-accept TTS license
      - AVATAR_BACKEND=${AVATAR_BACKEND:-auto}  # auto, sadtalker, liveportrait
    # Enable GPU access (for CUDA/NVIDIA)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - avatar-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Runtime Service (Orchestration + API)
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    container_name: realtime-avatar-runtime
    ports:
      - "8000:8000"
    volumes:
      # Share assets
      - ./runtime/assets:/app/assets:ro
      # Shared output with GPU service
      - gpu-output:/tmp/gpu-service-output:ro
      # Local output directory
      - ./runtime/outputs:/app/outputs
    environment:
      - MODE=local
      - DEVICE=cpu
      - LOG_LEVEL=info
      - USE_EXTERNAL_GPU_SERVICE=true
      - GPU_SERVICE_URL=http://gpu-service:8001
    depends_on:
      gpu-service:
        condition: service_healthy
    networks:
      - avatar-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Evaluator (runs on-demand)
  evaluator:
    build:
      context: ./evaluator
      dockerfile: Dockerfile
    container_name: realtime-avatar-evaluator
    volumes:
      - ./evaluator:/app:rw
      - ./runtime/assets:/app/assets:ro
      - ./evaluator/outputs:/app/outputs:rw
    environment:
      - RUNTIME_URL=http://runtime:8000
      - MODE=local
    depends_on:
      - runtime
    networks:
      - avatar-network
    profiles:
      - evaluator  # Only runs when explicitly called

networks:
  avatar-network:
    driver: bridge

volumes:
  model-cache:
    driver: local
  gpu-output:
    driver: local
