# Realtime Avatar: Interactive Conversational Avatar
Full-stack conversational AI avatar with voice input, intelligent responses, and realistic video synthesis.

https://github.com/user-attachments/assets/6b12fe94-5490-4aa5-a6ca-6c120d83b90b

https://github.com/user-attachments/assets/37ebcffc-f992-4499-bbd7-9e1762f6f611

```
+------------------------- Local Machine -------------------------+
|  Web UI (Nginx) - port 8080                                      |
|  User (browser)                                                  |
|  - record/playback, transcript                                   |
+-----------------------------------------------------------------+

                          HTTPS (public IP)
                     request  -->    <--  response
                                   v
+------------------------ GCP L4 GPU Instance --------------------+
|                                                                 |
|  +------------------------+                                     |
|  | Runtime API (FastAPI)  |                                     |
|  | port 8000              |                                     |
|  | - ASR (Whisper, CPU)   |                                     |
|  | - LLM (Gemini API)     |                                     |
|  |                        |                                     |
|  +-----------+------------+                                     |
|              | HTTP (req)                                       |
|              v                                                  |
|  +------------------------+       +---------------------------+ |
|  | GPU Service (FastAPI)  |       | Outputs (instance-local) |  |
|  | port 8001              |       | /tmp/gpu-service-output  |  |
|  | - XTTSâ€‘v2 (GPU)        | ----> | speech.wav, video.mp4    |  |
|  | - Ditto + TensorRT     |       +---------------------------+ |
|  +-----------^------------+                                     |
|              | HTTP (resp)                                      |
|              +---------------------> Runtime API                |
+-----------------------------------------------------------------+

```

Diagram Flow:
1. User records audio to initiatve chat.
2. SST: Text is transcribed to speech with Faster Whisper
3. LLM: A Conversation response is generated by Gemini 2.0
3. TTS: An audio response is generated with XTTS. The model is fine-tuned with the avatar character's voice (Bruce or Ming) ahead of time.
4. Video generation + Lip syncing is produced by: Ditto (from Alibaba)
5. Complete video is streamed to frontend in chunks to the frontend video player.

## ğŸ¯ Overview

**Status:** Phase 4 Deployed âœ… | **Performance:** TTS + Video Working on GCP L4

An end-to-end conversational avatar that:
- ğŸ¤ Listens to **voice input** (Whisper large-v3 ASR)
- ğŸ§  Responds intelligently (Qwen-2.5-7B LLM)
- ğŸ—£ï¸ Speaks in **cloned voice** (XTTS-v2 TTS)
- ğŸ­ Animates from **single photo** (Ditto + TensorRT)
- âš¡ **1.48x RTF video** generation (TensorRT acceleration)
- ğŸŒ **Web UI** with push-to-talk interface
- ğŸ”§ Hybrid deployment: Local web + Cloud GPU backend

## ğŸš€ Latest Performance (Nov 18, 2025)

**Full Pipeline Tested on GCP L4 GPU:**

| Component | Performance | Time (6.4s audio) |
|-----------|-------------|------------------|
| TTS (XTTS-v2) | **1.19x RTF** | ~8.4s |
| Video (Ditto TRT) | **1.48x RTF** | ~9.5s |
| **Combined** | End-to-end | **~18s** |

**Key Metrics:**
- Video generation: **1.48x RTF** with TensorRT âš¡
- TensorRT speedup: 2.5x vs PyTorch baseline
- GPU memory: ~7GB used (L4 has 24GB)
- Quality: Excellent lip sync and voice cloning âœ…

> **ğŸ’¡ Tip:** Check archived docs in `docs/archive/` for troubleshooting hints and implementation details

ğŸ“Š [Detailed benchmarks â†’](PERFORMANCE.md)

## ğŸ—ï¸ Architecture

**Phase 4: Full Conversation System**

```
User Voice Input â†’ ASR â†’ LLM â†’ TTS â†’ Video Synthesis â†’ Avatar Response
     (Web UI)    Whisper  Qwen  XTTS     Ditto TRT      (Playback)
```

**Components:**
- **ASR:** Whisper large-v3 (multilingual transcription)
- **LLM:** Qwen-2.5-7B (intelligent conversation)
- **TTS:** XTTS-v2 (voice cloning, EN/ZH/ES)
- **Video:** Ditto + TensorRT (audio-driven animation)
- **Web:** Nginx + vanilla JS (push-to-talk interface)

**Deployment:**
- **Local:** Web UI on Mac (Nginx container)
- **Cloud:** Backend on GCP L4 GPU (Docker containers)
- **Hybrid:** Local frontend â†’ Cloud backend over HTTP

ğŸ“– [Complete deployment guide â†’](DEPLOYMENT.md)

## ğŸš€ Quick Start

### One-Command Deployment

```bash
# Deploy everything (backend + web UI)
./deploy_phase4_hybrid.sh

# Open web interface
open http://localhost:8080
```

This script will:
1. Create/start GCP L4 GPU instance
2. Install Docker + NVIDIA drivers
3. Build and start backend containers
4. Configure and start local web UI

**That's it!** Start having conversations with your avatar.

ğŸ“– [Detailed deployment guide â†’](DEPLOYMENT.md)

### Manual Quick Start

**1. Deploy Backend to GCP:**
```bash
# Create L4 instance
gcloud compute instances create realtime-avatar-test \
  --zone=us-east1-c --machine-type=g2-standard-4 \
  --accelerator=type=nvidia-l4,count=1 \
  --boot-disk-size=100GB

# Deploy (see DEPLOYMENT.md for full steps)
```

**2. Start Web UI Locally:**
```bash
cd web
./configure_web.sh <INSTANCE_IP>
docker compose up -d
open http://localhost:8080
```

**3. Have a Conversation:**
- Click "Start Recording"
- Speak your message
- Click "Stop Recording"  
- Wait ~35-40s for avatar response
- Video plays automatically

## ğŸ“Š API Examples

**Full Conversation (Voice â†’ Video):**
```bash
# Record audio, get avatar video response
curl -X POST http://INSTANCE_IP:8000/api/v1/conversation \
  -F "audio=@my_question.wav" \
  --output avatar_response.mp4
```

**Individual Components:**

```bash
# Transcribe audio
curl -X POST http://INSTANCE_IP:8000/api/v1/transcribe \
  -F "audio=@audio.wav"
# â†’ {"text": "Hello, how are you?", "language": "en"}

# Chat (text â†’ text)
curl -X POST http://INSTANCE_IP:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Tell me about yourself"}'
# â†’ {"response": "I'm an AI assistant..."}

# Generate speech (text â†’ audio)  
curl -X POST http://INSTANCE_IP:8000/api/v1/speak \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world!", "language": "en"}' \
  --output speech.wav

# Generate video (audio â†’ video)
curl -X POST http://INSTANCE_IP:8000/api/v1/animate \
  -F "audio=@speech.wav" \
  --output video.mp4
```

## ğŸ¨ Features

**Phase 4 (Current):**
- âœ… Voice input with Whisper ASR
- âœ… Intelligent conversation with Qwen-2.5
- âœ… Voice cloning with XTTS-v2
- âœ… Video synthesis with Ditto TensorRT
- âœ… Web UI with push-to-talk
- âœ… Hybrid deployment model

**Phase 3:**
- âœ… TensorRT optimization (2.5x speedup)
- âœ… 2x RTF video generation
- âœ… Concurrent worker support

**Phase 2:**
- âœ… XTTS-v2 voice cloning
- âœ… Multilingual support (EN/ZH/ES)

**Phase 1:**
- âœ… Ditto avatar model integration
- âœ… Single photo animation

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [DEPLOYMENT.md](DEPLOYMENT.md) | **Complete deployment guide** (start here) |
| [PROJECT_STATUS.md](PROJECT_STATUS.md) | Current phase & achievements |
| [PERFORMANCE.md](PERFORMANCE.md) | Detailed benchmarks |
| [SETUP.md](SETUP.md) | Environment setup |

## ğŸ› ï¸ Development

**Requirements:**
- Python 3.10+ (GPU service) / 3.11+ (runtime)
- Docker & Docker Compose
- NVIDIA GPU (L4, A100, or similar)
- 100GB+ disk space
- 12GB+ GPU memory

**Local Development:**
```bash
# Setup environment
cd runtime
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Start GPU service
./run_gpu_service.sh &

# Start runtime
uvicorn app:app --reload

# Start web UI
cd ../web
docker compose up -d
```

**Testing:**
```bash
# Run API tests
pytest tests/

# Run benchmarks
python benchmark_simple.py

# Run evaluator
cd evaluator
python run_evaluator.py
```

## ğŸ’° Cost Estimate

**GCP L4 GPU (g2-standard-4):**
- **Running:** ~$0.80/hour
- **Stopped:** ~$16/month (disk only)
- **Monthly (24/7):** ~$600/month
- **Monthly (8hrs/day):** ~$200/month

**Optimization Tips:**
- Stop instance when not in use
- Use preemptible instances for testing
- Consider Cloud Run GPU for auto-scaling

## ğŸš§ Known Limitations

- First model load takes 30-60s (cold start)
- ~35-40s per conversation turn (optimization ongoing)
- Single concurrent user per instance
- Requires GPU with 12GB+ VRAM
- English works best, CJK languages experimental

## ğŸ”® Roadmap

**Near-term:**
- [ ] WebSocket streaming (reduce latency)
- [ ] Voice activity detection (hands-free)
- [ ] Multi-user support
- [ ] Cloud Run GPU deployment

**Future:**
- [ ] Real-time streaming (<5s latency)
- [ ] Custom avatar training
- [ ] Emotion detection & expression
- [ ] Multi-modal input (text + voice)

## ğŸ“„ License

MIT License - see LICENSE file

## ğŸ™ Acknowledgments

Built with:
- [Ditto](https://github.com/hkchengrex/Ditto) - Audio-driven video synthesis
- [XTTS-v2](https://github.com/coqui-ai/TTS) - Voice cloning
- [Whisper](https://github.com/openai/whisper) - Speech recognition
- [Qwen](https://github.com/QwenLM/Qwen) - Language model
- [TensorRT](https://developer.nvidia.com/tensorrt) - GPU acceleration

---

**Status:** Phase 4 Complete âœ…  
**Last Updated:** November 16, 2025  
**Performance:** ~35-40s per conversation turn

**Generate Spanish video:**
```bash
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{"text": "Â¡Hola mundo!", "language": "es"}'
```

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| **TTS** | XTTS-v2 (multilingual voice cloning) |
| **Avatar** | Ditto + TensorRT (audio-driven animation) |
| **ASR** | Faster-Whisper (Phase 3) |
| **API** | FastAPI (Python async) |
| **Deployment** | Docker + GCP L4 GPU |

## ğŸ“‚ Key Files

- `runtime/app.py` - FastAPI application
- `runtime/models/tts.py` - XTTS-v2 voice cloning
- `runtime/models/ditto_model.py` - Ditto avatar with TensorRT
- `runtime/gpu_service.py` - GPU acceleration service
- `runtime/pipelines/phase1_script.py` - Text â†’ video pipeline
- `evaluator/run_evaluator.py` - Automated testing

## ğŸ“š Documentation

- **[SETUP.md](SETUP.md)** - Complete setup guide (local + GCP)
- **[PERFORMANCE.md](PERFORMANCE.md)** - Benchmarks and optimization
- **[PROJECT_STATUS.md](PROJECT_STATUS.md)** - Current development status
- **[docs/TENSORRT_SETUP.md](docs/TENSORRT_SETUP.md)** - TensorRT installation

## ğŸ¯ Development Roadmap

- âœ… **Phase 1:** Script â†’ video (GPU accelerated)
- âœ… **Phase 2:** Voice cloning + optimization
- âœ… **Phase 3:** TensorRT (2.5x speedup)
- ğŸ“… **Phase 4:** Semi-interactive chat
- ğŸ“… **Phase 5:** Real-time WebRTC streaming

---

**Last Updated:** November 16, 2025 | **Status:** Phase 3 Complete âœ…
